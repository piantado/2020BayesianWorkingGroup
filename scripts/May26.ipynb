{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main assumption that we need to take on in Bayesian inference is that our hypotheses about the data are captured by a probability distribution. \n",
    "\n",
    "Take the following examples about flipping coins.\n",
    "\n",
    "$H_1 = P(Heads) = 0.5$\n",
    "\n",
    "$H_2 = P(Heads) = 0.75$\n",
    "\n",
    "$H_3 = P(Heads) = 0.25$\n",
    "\n",
    "For the example, assume $P(H_1) + P(H_2) + P(H_3) = 1$.\n",
    "\n",
    "More formally, we can express the main assumption for $n$ hypotheses as $\\sum_{i = 1}^{n} P(H_i) = 1$.\n",
    "\n",
    "Compared with frequentist analysis, which essentially has us look at the data as is, Bayesian inference tells us what to believe when we see the data. It combines two sources of information.\n",
    "\n",
    "1. *Priors*- what we believe before we see the data. This can be expressed as a distribution over hypotheses. \n",
    "A *uniform* prior is $P(H_1) = P(H_2) = P(H_3) = \\frac{1}{3}$. If we express a uniform prior over $n$ hypotheses, the probability of each hypothesis is $\\frac{1}{n}$. A lot of the time, we may think that some hypotheses are more likely than others, so another prior could be the following: $P(H_1) = 0.9$ (we think the coin is most likely fair) and $P(H_2) = P(H_3) = 0.05$. This could also be expressed as $P(H_1) = v$ and $P(H_2) = P(H_3) = (1 - v) / 2$, where $v = 0.9$.\n",
    "\n",
    "In the next cell, we show a uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = np.array([1/3, 1/3, 1/3]) #H1, H2, and H3 have the same prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. *Likelihoods*- How well any hypothesis explains the data. Technically, this is what probability the hypothesis assigns to the data, or $P(D | H)$. In the next cell, let's assume we've observed 3 coin tosses {H, H, T}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nH = 2 #number of heads\n",
    "nT = 1 #number of tails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the likelihoods under each hypothesis. Coin tosses follow a binomial distribution, so each item in the `likelihood` array shows the probability of 2/3 tosses being heads based on the coin's probability under the corresponding hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.375   , 0.421875, 0.140625])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood = np.array([stats.binom.pmf(nH, nH + nT, 0.5), #H1\n",
    " stats.binom.pmf(nH, nH + nT, 0.75), #H2\n",
    " stats.binom.pmf(nH, nH + nT, 0.25)]) #H3\n",
    "likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' Rule says that  $P(H_1 | data) \\propto P(data | H_1) P(H_1)$, or generally that the probability of the hypothesis given the data is proportional to the likelihood times the prior, computed for that specific hypothesis.\n",
    "\n",
    "We derive Bayes' Rule from the definition of conditional probability/the division rule.\n",
    "\n",
    "$P(A|B) = \\frac{P(A,B)}{P(B)}$\n",
    "\n",
    "$P(A,B) = P(A|B) P(B)$\n",
    "\n",
    "$P(B,A) = P(B|A) P(A)$\n",
    "\n",
    "From the previous two lines, $P(A|B) P(B) = P(B|A) P(A)$\n",
    "\n",
    "$\\frac{P(A|B) P(B)}{P(A)} = P(B|A)$, so $P(B|A) \\propto P(A|B) P(B)$.\n",
    "\n",
    "Expanding this to hypotheses and data, we get the equation at the beginning of the cell, as well as: \n",
    "$P(H_1 | data) = \\frac{P(data | H_1) P(H_1)}{P(data)}$.\n",
    "\n",
    "But what is $P(data)$? Summing over the hypotheses, we compute the marginal probability. $P(data) = \\sum_{i = 1}^{n} P(H_i) P(data | H_i)$.\n",
    "\n",
    "Sometimes it's hard to compute, so in those cases we just express the prior as proportional to the product of the likelihood and posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4 , 0.45, 0.15])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior = prior * likelihood / sum(prior * likelihood)\n",
    "posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dutch book argument: \"rational people must have subjective probabilities for random events, and that these probabilities must satisfy the standard axioms of probability\" (from Wikipedia, please add more).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
